{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6885180f-0fd6-47da-9163-df58aa74bd71",
    "_uuid": "e150e6dc-6701-44b7-a430-e2a4e2ae3e0a"
   },
   "source": [
    "# Deep and Wide Network Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many diseases, including cancer, are believed to have a contributing factor in common. Ion channels are pore-forming proteins present in animals and plants. They encode learning and memory, help fight infections, enable pain signals, and stimulate muscle contraction. If scientists could better study ion channels, which may be possible with the aid of machine learning, it could have a far-reaching impact.\n",
    "\n",
    "When ion channels open, they pass electric currents. Existing methods of detecting these state changes are slow and laborious. Humans must supervise the analysis, which imparts considerable bias, in addition to being tedious. These difficulties limit the volume of ion channel current analysis that can be used in research. Scientists hope that technology could enable rapid automatic detection of ion channel current events in raw data.\n",
    "\n",
    "Technology to analyze electrical data in cells has not changed significantly over the past 20 years. If we better understand ion channel activity, the research could impact many areas related to cell health and migration. From human diseases to how climate change affects plants, faster detection of ion channels could greatly accelerate solutions to major world problems.\n",
    "\n",
    "<img src=\"resources/channels.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e2b475f3-986b-40d7-b0b0-1fa15bdd0a1a",
    "_uuid": "015bca17-0a11-4d85-a9c8-cb9f99823d24"
   },
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    IS_COLAB = True\n",
    "except Exception:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "if not tf.test.is_gpu_available():\n",
    "    print(\"No GPU was detected. Neural Nets can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "seed = 42\n",
    "        \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "import numpy as np \n",
    "import warnings\n",
    "import gc\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7c18bfe6-cefb-4b4a-9947-b92c1eaad8c8",
    "_uuid": "4b11e7d9-d8c4-442e-88ed-d2fbb9bddc68"
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    if os.path.isdir('/kaggle/input/'):\n",
    "        path_str = '/kaggle/input/'\n",
    "    else:\n",
    "        path_str = '/input/'\n",
    "    '''Read the data'''\n",
    "    print('Reading training, testing and submission data...')\n",
    "    train = pd.read_csv(path_str+'liverpool-ion-switching/train.csv')\n",
    "    test = pd.read_csv(path_str+'liverpool-ion-switching/test.csv')\n",
    "    submission = pd.read_csv(path_str+'liverpool-ion-switching/sample_submission.csv', dtype={'time':str})\n",
    "    print('Train set has {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n",
    "    print('Test set has {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n",
    "    return train, test, submission\n",
    "\n",
    "train, test, submission = read_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "39f10b64-d112-41c4-801b-4d1c6f3db95c",
    "_uuid": "1443f73a-8179-4a1d-b79e-798dabb82009"
   },
   "source": [
    "From the exploratory data analysis we know that we have 10 batches for training and 4 batches in the test. The batches are independent from each other so our job is to extract features in those batches to improve the network performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1072d102-ad55-4183-ad17-48e0254e3209",
    "_uuid": "115e73e6-a716-42ec-9c97-25902815aa70"
   },
   "outputs": [],
   "source": [
    "# concatenate data\n",
    "batch = 50\n",
    "total_batches = 14\n",
    "train['set'] = 'train'\n",
    "test['set'] = 'test'\n",
    "data = pd.concat([train, test])\n",
    "for i in range(int(total_batches)):\n",
    "    data.loc[(data['time'] > i * batch) & (data['time'] <= (i + 1) * batch), 'batch'] = i + 1\n",
    "train = data[data['set'] == 'train']\n",
    "test = data[data['set'] == 'test']\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "31be7eed-cc66-401f-825c-5dabe16a69ab",
    "_uuid": "5bcb718d-f7f5-4baf-9a30-159ceaec4ef1",
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ef62f6efcca9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# feature engineering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mpre_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;31m# reduce memory usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# scaling and filling missing values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "def preprocess(train, test):\n",
    "    ''' Data preprocessing to extract mean, min, max and std''' \n",
    "    pre_train = train.copy()\n",
    "    pre_test = test.copy()\n",
    "    \n",
    "    batch1 = pre_train[pre_train[\"batch\"] == 1]\n",
    "    batch2 = pre_train[pre_train[\"batch\"] == 2]\n",
    "    batch3 = pre_train[pre_train[\"batch\"] == 3]\n",
    "    batch4 = pre_train[pre_train[\"batch\"] == 4]\n",
    "    batch5 = pre_train[pre_train[\"batch\"] == 5]\n",
    "    batch6 = pre_train[pre_train[\"batch\"] == 6]\n",
    "    batch7 = pre_train[pre_train[\"batch\"] == 7]\n",
    "    batch8 = pre_train[pre_train[\"batch\"] == 8]\n",
    "    batch9 = pre_train[pre_train[\"batch\"] == 9]\n",
    "    batch10 = pre_train[pre_train[\"batch\"] == 10]\n",
    "    batch11 = pre_test[pre_test['batch'] == 11]\n",
    "    batch12 = pre_test[pre_test['batch'] == 12]\n",
    "    batch13 = pre_test[pre_test['batch'] == 13]\n",
    "    batch14 = pre_test[pre_test['batch'] == 14]\n",
    "    batches = [batch1, batch2, batch3, batch4, batch5, batch6, batch7, batch8, batch9, batch10, batch11, batch12, batch13, batch14]\n",
    "    \n",
    "    for batch in batches:\n",
    "        for feature in ['signal']:\n",
    "            # some random rolling features\n",
    "            for window in [100, 1000, 10000]:\n",
    "                # roll backwards\n",
    "                batch[feature + 'mean_t' + str(window)] = batch[feature].shift(1).rolling(window).mean()\n",
    "                batch[feature + 'std_t' + str(window)] = batch[feature].shift(1).rolling(window).std()\n",
    "                batch[feature + 'min_t' + str(window)] = batch[feature].shift(1).rolling(window).min()\n",
    "                batch[feature + 'max_t' + str(window)] = batch[feature].shift(1).rolling(window).max()\n",
    "                min_max = (batch[feature] - batch[feature + 'min_t' + str(window)]) / (batch[feature + 'max_t' + str(window)] - batch[feature + 'min_t' + str(window)])\n",
    "                batch['norm_t' + str(window)] = min_max * (np.floor(batch[feature + 'max_t' + str(window)]) - np.ceil(batch[feature + 'min_t' + str(window)]))\n",
    "                \n",
    "    pre_train = pd.concat([batch1, batch2, batch3, batch4, batch5, batch6, batch7, batch8, batch9, batch10])\n",
    "    pre_test = pd.concat([batch11, batch12, batch13, batch14])\n",
    "    \n",
    "    del batches, batch1, batch2, batch3, batch4, batch5, batch6, batch7, batch8, batch9, batch10, batch11, batch12, batch13, batch14, train, test, min_max\n",
    "    \n",
    "    return pre_train, pre_test\n",
    "\n",
    "\n",
    "def scale_fillna(pre_train, pre_test):\n",
    "    '''Fill NA values that were created in the preprocess'''\n",
    "    features = [col for col in pre_train.columns if col not in ['open_channels', 'set', 'time', 'batch']]\n",
    "    pre_train = pre_train.replace([np.inf, -np.inf], np.nan)\n",
    "    pre_test = pre_test.replace([np.inf, -np.inf], np.nan)\n",
    "    pre_train.fillna(0, inplace = True)\n",
    "    pre_test.fillna(0, inplace = True)\n",
    "\n",
    "    return pre_train, pre_test\n",
    "\n",
    "# feature engineering\n",
    "pre_train, pre_test = preprocess(train, test)\n",
    "# reduce memory usage\n",
    "# scaling and filling missing values\n",
    "pre_train, pre_test = scale_fillna(pre_train, pre_test)\n",
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "51bf6c51-b2b3-48f1-8fc8-6468c7b8e274",
    "_uuid": "d2386cc3-dd79-4567-95c4-d18ed2d6838a"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def split_data(df, features):\n",
    "    '''Shuffle the data and split into training and validation data'''\n",
    "    split1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=97)\n",
    "    for train_index, valid_index in split1.split(df, df['open_channels']):\n",
    "        strat_train_set = df.loc[train_index]\n",
    "        strat_valid_set = df.loc[valid_index]\n",
    "\n",
    "    X_train, y_train = strat_train_set[features], strat_train_set['open_channels']\n",
    "    X_valid, y_valid = strat_valid_set[features], strat_valid_set['open_channels']\n",
    "    \n",
    "    return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We developed a simple network but taking the input also as an input of the final layers, this way the netwwork learns both deep patterns and simple rules. It was introduced by Heng-Tze Cheng as a Wide and Deep Neural Network.\n",
    "\n",
    "<img src=\"resources/image.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e21ec3af-f7eb-4761-aaf7-916d6c26457a",
    "_uuid": "ff4df3c6-5f32-4430-be36-1cecfa1c46a6"
   },
   "outputs": [],
   "source": [
    "def create_model(X_train):\n",
    "    '''Creates a new model based on the training instance'''\n",
    "    from keras import backend as K\n",
    "    from keras import metrics\n",
    "\n",
    "    K.clear_session()\n",
    "    neurons = 50\n",
    "\n",
    "    input_A = keras.layers.Input(shape=X_train.shape[1:])\n",
    "    hidden1 = keras.layers.Dense(neurons, activation=\"selu\", kernel_initializer=\"lecun_normal\")(input_A)\n",
    "    hidden2 = keras.layers.Dense(neurons, activation=\"selu\", kernel_initializer=\"lecun_normal\")(hidden1)\n",
    "    hidden3 = keras.layers.Dense(neurons, activation=\"selu\", kernel_initializer=\"lecun_normal\")(hidden2)\n",
    "    concat = keras.layers.concatenate([input_A, hidden3])\n",
    "    output = keras.layers.Dense(11, activation=\"softmax\")(concat)\n",
    "\n",
    "    model = keras.models.Model(inputs=[input_A], outputs=[output])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "188dd5f1-eb22-4208-8c9d-2c3193a06f91",
    "_uuid": "ae0ae7f4-1cfe-434f-808b-146d1a0d5563"
   },
   "outputs": [],
   "source": [
    "def train_model(model, X_train, X_valid, y_train, y_valid):\n",
    "    '''Trains the model using Deep & Wide approach'''\n",
    "    from sklearn.utils import class_weight\n",
    "    \n",
    "    lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.7, patience=3)\n",
    "\n",
    "    class_weight_values = class_weight.compute_class_weight('balanced'\n",
    "                                                   ,np.unique(y_train)\n",
    "                                                   ,y_train)\n",
    "\n",
    "\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(\"Deep_Wide.h5\", save_best_only=True)\n",
    "    early_stopping_cb = keras.callbacks.EarlyStopping(patience=7, restore_best_weights=True)\n",
    "\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                 optimizer=\"nadam\",\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(X_train, y_train, epochs=30,batch_size=16,class_weight=class_weight_values, \n",
    "                       validation_data=(X_valid, y_valid), callbacks=[lr_scheduler, checkpoint_cb, early_stopping_cb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e7cd326e-e63a-4e9f-94fe-efd123f1081b",
    "_uuid": "6daf6c8c-c7ca-4c8e-9ea4-f65bc25acda4"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def score(X):\n",
    "    '''Calculates F1 score on the validation data'''\n",
    "    Y_pred = model.predict(X).argmax(axis=1)\n",
    "    score = f1_score(y_valid, Y_pred, average=\"macro\")\n",
    "    print ('F1 Score (Neural Network): '+str(score))\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e9c31c88-fb3e-41a7-b98e-6d0f5d176023",
    "_uuid": "f35643a1-fe8c-4787-8489-ab6ccda212be"
   },
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    model.save(\"models/Deep_Wide.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3cf2abaf-c161-4bb5-adc6-088b23a14ace",
    "_uuid": "d44ac510-ae72-4beb-b8a5-7c85743e35c0"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "features = [col for col in pre_train.columns if col not in ['open_channels', 'set', 'time', 'batch']] # define the features for training\n",
    "X_train, X_valid, y_train, y_valid = split_data(pre_train, features)\n",
    "   \n",
    "scaler = StandardScaler()\n",
    "scale = scaler.fit(X_train)\n",
    "X_train = scale.transform(X_train)\n",
    "X_valid = scale.transform(X_valid)\n",
    "X_test = scale.transform(pre_test[features])\n",
    "                                           \n",
    "if os.path.exists('/models/Deep_Wide.h5'):\n",
    "      model = keras.models.load_model('/models/Deep_Wide.h5')\n",
    "elif os.path.exists('/kaggle/input/models/Deep_Wide.h5'):\n",
    "    model = keras.models.load_model('/kaggle/input/models/Deep_Wide.h5')\n",
    "else:\n",
    "    model = create_model(X_train)\n",
    "    train_model(model, X_train, X_valid, y_train, y_valid)\n",
    "\n",
    "save_model(model)\n",
    "score(X_valid)\n",
    "\n",
    "y_pred = model.predict(X_test).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8fdc39cf-56c1-4d5d-8c4f-5ee9f2723ba1",
    "_uuid": "d7e811c6-dd83-4c08-a8fe-130c6f4d0210"
   },
   "outputs": [],
   "source": [
    "submission.open_channels = y_pred\n",
    "submission.to_csv(\"submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
