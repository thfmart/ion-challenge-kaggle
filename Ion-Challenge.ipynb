{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    IS_COLAB = True\n",
    "except Exception:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "if not tf.test.is_gpu_available():\n",
    "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "seed = UserSecretsClient().get_secret(\"seed_label\")\n",
    "        \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(97)\n",
    "tf.random.set_seed(97)\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pre-Process Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following steps we import the data provided by the competition and prepare the Standart Scaler to use in Neural Networks.\n",
    "We also separate the data into training and validation to check the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/liverpool-ion-switching/train.csv', low_memory=False)\n",
    "df.dropna()\n",
    "\n",
    "scaler = Pipeline([\n",
    "     ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()\n",
    "corr_matrix[\"open_channels\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=97)\n",
    "for train_index, valid_index in split1.split(df, df['open_channels']):\n",
    "    strat_train_set = df.loc[train_index]\n",
    "    strat_valid_set = df.loc[valid_index]\n",
    "    \n",
    "X_train, y_train = strat_train_set.drop(['open_channels'],axis=1), strat_train_set['open_channels']\n",
    "X_valid, y_valid = strat_valid_set.drop(['open_channels'],axis=1), strat_valid_set['open_channels']\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split2 = StratifiedShuffleSplit(n_splits=1, train_size=0.1, test_size=0.05, random_state=97)\n",
    "for train_index, valid_index in split2.split(df, df['open_channels']):\n",
    "    strat_red_train_set = df.loc[train_index]\n",
    "    strat_red_valid_set = df.loc[valid_index] \n",
    "\n",
    "\n",
    "X_train, y_train = strat_red_train_set.drop(['open_channels'],axis=1), strat_red_train_set['open_channels']\n",
    "X_valid, y_valid = strat_red_valid_set.drop(['open_channels'],axis=1), strat_red_valid_set['open_channels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = scaler.fit(X_train)\n",
    "X_train_scale = scale.transform(X_train)\n",
    "X_valid_scale = scale.transform(X_valid)\n",
    "X_train_scale[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide and Deep Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We developed a simple network but taking the input also as an input of the final layers, this way the netwwork learns both deep patterns and simple rules. It was introduced by Heng-Tze Cheng as a Wide and Deep Neural Network.\n",
    "\n",
    "<img src=\"files/Resources/image.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    from keras import backend as K\n",
    "    from keras import metrics\n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    # scaled = scaler.fit(X_train)\n",
    "    # X_train = scaled.transform(X_train)\n",
    "    # X_valid = scaled.transform(X_valid)\n",
    "\n",
    "    input_A = keras.layers.Input(shape=X_train_scale.shape[1:])\n",
    "    hidden1 = keras.layers.Dense(500, activation=\"selu\", kernel_initializer=\"lecun_normal\")(input_A)\n",
    "    hidden2 = keras.layers.Dense(500, activation=\"selu\", kernel_initializer=\"lecun_normal\")(hidden1)\n",
    "    hidden3 = keras.layers.Dense(500, activation=\"selu\", kernel_initializer=\"lecun_normal\")(hidden2)\n",
    "    concat = keras.layers.concatenate([input_A, hidden3])\n",
    "    output = keras.layers.Dense(11, activation=\"softmax\")(concat)\n",
    "\n",
    "    model = keras.models.Model(inputs=[input_A], outputs=[output])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    from sklearn.utils import class_weight\n",
    "    \n",
    "    lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.7, patience=3)\n",
    "\n",
    "    class_weight_values = class_weight.compute_class_weight('balanced'\n",
    "                                                   ,np.unique(y_train)\n",
    "                                                   ,y_train)\n",
    "\n",
    "\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(\"DNN_model.h5\", save_best_only=True)\n",
    "    early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",#sparse_categorical_crossentropy (Classes 1 to 10), binary_crossentropy, huber_loss->continous\n",
    "                 optimizer=\"nadam\",\n",
    "                 metrics=['accuracy'])#'sparse_top_k_categorical_accuracy'])#[metrics.mae])#[\"accuracy\"])# ,\"categorical_accuracy\", sp_precision, sp_recall\n",
    "\n",
    "    history = model.fit(X_train_scale, y_train, epochs=5,batch_size=16,class_weight=class_weight_values, \n",
    "                       validation_data=(X_valid_scale, y_valid), callbacks=[lr_scheduler, checkpoint_cb, early_stopping_cb])\n",
    "\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    Y_pred = model.predict(X_valid_scale).argmax(axis=1)\n",
    "    score = f1_score(y_valid, Y_pred, average=\"macro\")\n",
    "\n",
    "    print ('F1 Score: '+str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def print_score():\n",
    "    Y_pred = model.predict(X_valid_scale).argmax(axis=1)\n",
    "    score = f1_score(y_valid, Y_pred, average=\"macro\")\n",
    "\n",
    "    print ('F1 Score (Neural Network): '+str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    model.save(\"NN.h5\")\n",
    "    #model.save(\"models/NN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('/kaggle/input/NN.h5'):\n",
    "    model = keras.models.load_model('/kaggle/input/models/NN.h5')\n",
    "else:\n",
    "    model = create_model()\n",
    "    train_model(model)\n",
    "    print_score()\n",
    "    save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LGBM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following commands creates, train and save a LGBM classifier, which is a boost algorithm based on decision trees.\n",
    "\n",
    "[Documentation](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def LGBM_create():\n",
    "    gb=lgb.LGBMClassifier(\n",
    "        boosting_type= 'gbdt',\n",
    "        objective = 'multiclass',\n",
    "        metric = 'multi_logloss',\n",
    "        num_class = 11,\n",
    "        num_leaves = 200,#1000\n",
    "        bagging_freq = 60, #50\n",
    "        max_depth = 200, #300\n",
    "        learning_rate = 0.1,\n",
    "        feature_fraction = 0.1,\n",
    "        bagging_fraction = 0.8,\n",
    "        n_estimators = 150,\n",
    "        min_data_in_leaf = 500, #1000\n",
    "        max_bin=5000,\n",
    "        verbose = 0)\n",
    "    return gb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LGBM_train(gb):\n",
    "    gb.fit(X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        eval_metric='multi_logloss',\n",
    "        early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def LGBM_score():\n",
    "    y_pred = gb.predict(X_valid)\n",
    "    score = f1_score(y_valid, y_pred, average=\"macro\")\n",
    "    print ('F1 Score (LGBM): '+str(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LGBM_save():\n",
    "    #gb.booster_.save_model('models/LGBM.txt')\n",
    "    gb.booster_.save_model('LGBM.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('/kaggle/input/models/LGBM.txt'):\n",
    "    gb = lgb.Booster(model_file='/kaggle/input/models/LGBM.txt')\n",
    "else:\n",
    "    gb = LGBM_create()\n",
    "    LGBM_train(gb)\n",
    "    LGBM_score()\n",
    "    LGBM_save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Stacking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a esemble (with a weighted average) of the 2 classifiers.\n",
    "The weights are calculated through a custom Grid Search, where we look for the combination of parameters that increase F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying creating a Stack with the best algorithms and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "strat_valid_set = strat_valid_set.reset_index(drop=True)\n",
    "\n",
    "split2 = StratifiedShuffleSplit(n_splits=1, train_size=0.8, random_state=100)\n",
    "for train_index, valid_index in split2.split(strat_valid_set, strat_valid_set['open_channels']):\n",
    "    strat_valid_train = strat_valid_set.loc[train_index]\n",
    "    strat_valid_valid = strat_valid_set.loc[valid_index]\n",
    "    \n",
    "    \n",
    "X_t, y_t = strat_valid_train.drop(['open_channels'], axis=1), strat_valid_train['open_channels']\n",
    "X_v, y_v = strat_valid_valid.drop(['open_channels'],axis=1), strat_valid_valid['open_channels']\n",
    "\n",
    "X_t_scale = scale.transform(X_t)\n",
    "X_v_scale = scale.transform(X_v)\n",
    "\n",
    "#X_pred = np.concatenate(([model.predict(X_t_scale).argmax(axis=1)],[gb.predict(X_t).argmax(axis=1)]), axis=0).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def find_weights():\n",
    "\n",
    "    weight_gb = np.empty(len(np.unique(y_t)), dtype=np.float32)\n",
    "    weight_model = np.empty(len(np.unique(y_t)), dtype=np.float32)\n",
    "\n",
    "    f1_score_matrix = np.empty((11,11), dtype=np.float32)\n",
    "    for i in range (len(weight_gb)):\n",
    "        weight_gb[i] = 0.5\n",
    "        weight_model[i] = 0.5\n",
    "\n",
    "\n",
    "    for k in range(11):\n",
    "        for l in range(11):\n",
    "            weight_value = k/10\n",
    "            weight_gb[l] = weight_value\n",
    "            weight_model[l] = 1-weight_value\n",
    "            print (str(k) + ' ' + str(l))\n",
    "            y_p = (gb.predict_proba(X_t)*weight_gb + model.predict(X_t_scale)*weight_model).argmax(axis=1)\n",
    "            f1_score_matrix[k][l] = f1_score(y_t, y_p, average=\"macro\")\n",
    "            weight_gb[l] = 0.5\n",
    "            weight_model[l] = 0.5\n",
    "\n",
    "    f1_score_matrix\n",
    "    weight = f1_score_matrix.argmax(axis=0)/10\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "#weight = np.array([1. , 1. , 0.7, 0.7, 0.8, 0.9, 0.3, 0.3, 0.6, 0.9, 0.9])\n",
    "weight = find_weights()\n",
    "weight_gb = weight\n",
    "weight_model = 1-weight\n",
    "y_p = (gb.predict(X_valid)*weight_gb + model.predict(scale.transform(X_valid))*weight_model).argmax(axis=1)\n",
    "score = f1_score(y_valid, y_p, average=\"macro\")\n",
    "print ('F1 Score: '+str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the confusion matrix for the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf_mx = confusion_matrix(y_valid.to_numpy().ravel(), y_pred)\n",
    "plt.matshow(conf_mx, cmap = plt.cm.gray)\n",
    "plt.show()\n",
    "#Row = Actual Classes, Column = Predicted Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
    "plt.show()\n",
    "#Row = Actual Classes, Column = Predicted Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Test Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we use the model to make the predictions on the test data and submit the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
